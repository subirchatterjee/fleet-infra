apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: longhorn
  namespace: flux-system
spec:
  targetNamespace: longhorn-system
  releaseName: longhorn
  chart:
    spec:
      chart: longhorn
      version: "1.5.1"
      sourceRef:
        kind: HelmRepository
        name: longhorn
        namespace: flux-system
  interval: 2m0s
  values:
    defaultSettings:
      # # see longhorn-metrics.yaml in this repo for an example
      # # how to disable collecting usage metrics
      # allowRecurringJobWhileVolumeDetached: false # default
      # autoCleanupSystemGeneratedSnapshot: true # default
      # autoDeletePodWhenVolumeDetachedUnexpectedly: true # default
      # autoSalvage: true # default
      # # backupTarget: s3://${longhorn_backup_bucket}@us-east-1/
      # # backupTargetCredentialSecret: longhorn-takeout-backups
      # # backupstorePollInterval: 0 # seconds
      # concurrentAutomaticEngineUpgradePerNodeLimit: 3
      # concurrentReplicaRebuildPerNodeLimit: 10 # double the default, 5
      # concurrentVolumeBackupRestorePerNodeLimit: 5 # default
      # createDefaultDiskLabeledNodes: true
      # # This is tempting to set to true, but I am not okay with trading
      # # confidence about which volume is the best in a rebuild for more
      # # performance. Picking the biggest replica is naiive, my cluster performs
      # # well enough. See: https://longhorn.io/docs/1.4.0/advanced-resources/deploy/revision_counter/
      # disableRevisionCounter: false
      # disableSchedulingOnCordonedNode: true # default
      defaultDataPath: /mnt/longhorn
      defaultDataLocality: best-effort
      defaultLonghornStaticStorageClass: longhorn
      defaultReplicaCount: 3 # default
      engineReplicaTimeout: 8 # default
      failedBackupTTL: 1440 # default
      # fastReplicaRebuildEnabled: true # relies on snapshotDataIntegrity to be enable or fast-check
      # guaranteedInstanceManagerCPU: 20 # percent
      # nodeDownPodDeletionPolicy: delete-both-statefulset-and-deployment-pod
      # offlineReplicaRebuilding: true # default
      # orphanAutoDeletion: true # This deletes anything not Longhorn in the data directory. Be careful.
      # replicaReplenishmentWaitInterval: 90 # 15% of default (600), in seconds
      # removeSnapshotsDuringFilesystemTrim: false # default
      # restoreVolumeRecurringJobs: false # default - I use my longhorn-recurring-jobs chart instead
      # snapshotDataIntegrity: fast-check # default
      # snapshotDataIntegrityCronjob: 0 8 */7 * * # 4AM for EST
      # snapshotDataIntegrityImmediateCheckAfterSnapshotCreation: false # default
      # storageMinimalAvailablePercentage: 10
      # storageOverProvisioningPercentage: 100
      # storageReservedPercentageForDefaultDisk: 0
      # upgradeChecker: false # I watch for Longhorn releases
      # v2DataEngine: false # default, disabled
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: longhorn-oauth2-proxy
  namespace: longhorn-system
spec:
  targetNamespace: longhorn-system
  releaseName: longhorn-oauth2-proxy
  chart:
    spec:
      chart: oauth2-proxy
      version: "6.17.1"
      sourceRef:
        kind: HelmRepository
        name: oauth2-proxy
        namespace: flux-system
  interval: 2m0s
  values:
    replicaCount: 1
    extraArgs:
      provider: github
      github-org: "${blog_name}"
      github-team: "super-admins,spartacus-longhorn"
      cookie-expire: 168h0m0s
      scope: "user:email"
    config:
      existingSecret: longhorn-oauth2-proxy
